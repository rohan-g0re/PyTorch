{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g6_X9BBp5a6G",
        "kwvnfoGX52fk",
        "xU9uxsai550Y",
        "Beb02V9u6K0d",
        "j29BPEEi6QVl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Getting Started Tips\n",
        "\n",
        "1. **Start Small**: Begin with CIFAR-10 instead of ImageNet for faster iteration\n",
        "2. **Test Each Phase**: Run verification functions after implementing each phase\n",
        "3. **Debug Shapes**: Print tensor shapes frequently to catch dimension mismatches early\n",
        "4. **Use Small Batches**: Start with small batch sizes to avoid memory issues\n",
        "\n",
        "Work through each function stub systematically. The hints give you the conceptual understanding, but you'll need to research the specific PyTorch APIs and mathematical implementations. Come back with questions about specific functions when you get stuck!\n",
        "\n"
      ],
      "metadata": {
        "id": "xR1XqUe26aYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 1: Environment Setup\n",
        "\n",
        "**Detailed Hint:** You need to establish your development environment with the right deep learning framework. Think about what libraries you'll need for neural networks, computer vision operations, mathematical computations, and data handling. Also consider GPU support if available. The framework choice will determine your entire implementation approach - PyTorch tends to be more research-friendly and closer to how papers describe things.\n"
      ],
      "metadata": {
        "id": "b0SnUniJ5RXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# TODO: Fill in the necessary imports\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Set up all the necessary imports and check for GPU availability.\n",
        "    Hint: You'll need torch, torchvision, numpy, and possibly matplotlib for visualization.\n",
        "    \"\"\"\n",
        "    # Import statements go here\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import torchvision\n",
        "    import torchvision.transforms as transforms\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    device = None  # TODO: Determine if you should use 'cuda' or 'cpu'\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "def set_random_seeds(seed=42):\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility across different libraries.\n",
        "    Hint: Neural networks involve randomness in initialization, data shuffling, etc.\n",
        "    You want consistent results across runs for debugging.\n",
        "    \"\"\"\n",
        "    # TODO: Set seeds for torch, numpy, and random module\n",
        "\n",
        "    torch.manual_seed(seed)                    # PyTorch CPU random numbers\n",
        "    torch.cuda.manual_seed(seed)               # PyTorch GPU random numbers\n",
        "    torch.cuda.manual_seed_all(seed)           # For multi-GPU setups\n",
        "    np.random.seed(seed)                       # NumPy random numbers\n",
        "    random.seed(seed)\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "setup_environment()\n",
        "set_random_seeds()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhLpV_0o5Rwy",
        "outputId": "7c4153a8-c5bf-4fec-cd8d-ac638d4e3757"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Phase 2: Data Preprocessing \\& Augmentation\n",
        "\n",
        "**Detailed Hint:** AlexNet's power comes partly from its data augmentation strategy. You need to think about how to transform images during training vs testing. During training, you want randomness (different crops, flips) to artificially expand your dataset. During testing, you want consistency and thoroughness (systematic crops). The original paper mentions specific image sizes: input images are 256×256, but the network expects 224×224 patches. Consider what happens to the \"extra\" 32 pixels on each side."
      ],
      "metadata": {
        "id": "g6_X9BBp5a6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_basic_transforms():\n",
        "    \"\"\"\n",
        "    Create the basic image transformations for AlexNet.\n",
        "    Hint: Think about the paper's mention of 256×256 input images and 224×224 patches.\n",
        "    What mathematical operations convert images to the right format for neural networks?\n",
        "    \"\"\"\n",
        "    # TODO: Compose transformations for training\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "\n",
        "         transforms.Resize(256),\n",
        "         transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "         #as 3 transformations were listed in paper, we have done all three --> now convert to tensor and move ahead\n",
        "         transforms.ToTensor(),\n",
        "\n",
        "\n",
        "         #normalization is apparrently a standard practice --> ON THE OTHER HAND,\n",
        "         # the numbers chosen are a standard practice for ImageNet\n",
        "\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Compose transformations for validation/testing\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "\n",
        "        #we choose centre crop bcoz it is deterministic which is what we want in validation set -->\n",
        "        #or else each time our val acc will be different due to flips and randomk crops -->\n",
        "        #hence we also skop the flips\n",
        "\n",
        "        transforms.CenterCrop(224),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n"
      ],
      "metadata": {
        "id": "xigpmzwi5cmV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_ten_test_crops(image_tensor):\n",
        "    \"\"\"\n",
        "    Extract the 10 crops used during AlexNet testing phase.\n",
        "    Hint: The paper mentions \"four corner patches and the center patch\" plus their horizontal reflections.\n",
        "    Think about where these 5 locations would be in a 256×256 image when extracting 224×224 patches.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Input: image_tensor: PyTorch tensor of shape [C, H, W]\n",
        "    --> where C (Channels) =3 (RGB)\n",
        "    -->  H (Height) = W (Width) = 256\n",
        "\n",
        "    Output: list of 10 image tensors of shape [C, 224, 224]\n",
        "    \"\"\"\n",
        "    crops = []\n",
        "\n",
        "    # Verifying image_tensor shape is correct\n",
        "\n",
        "    assert image_tensor.dim() == 3, f\"Expected 3D tensor [C,W,H], got {image_tensor.dim()} D\"\n",
        "\n",
        "    assert image_tensor.shape[-2:] == (256, 256), f\"Expected 256x256 image, got with shape {image_tensor.shape[-2:]} \"\n",
        "\n",
        "    # TODO: Extract 4 corner crops (top-left, top-right, bottom-left, bottom-right)\n",
        "\n",
        "\n",
        "# first is colon bcoz we are taking all channels (RGB)\n",
        "\n",
        "    top_left = image_tensor[:, 0:224, 0:224]\n",
        "    crops.append(top_left)\n",
        "\n",
        "    # Top-right corner crop\n",
        "    top_right = image_tensor[:, 0:224, 32:256]  # 256-224=32, so start at col 32\n",
        "    crops.append(top_right)\n",
        "\n",
        "    # Bottom-left corner crop\n",
        "    bottom_left = image_tensor[:, 32:256, 0:224]  # Start at row 32\n",
        "    crops.append(bottom_left)\n",
        "\n",
        "    # Bottom-right corner crop\n",
        "    bottom_right = image_tensor[:, 32:256, 32:256]  # Start at (32, 32)\n",
        "    crops.append(bottom_right)\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Extract center crop\n",
        "\n",
        "    center = image_tensor[:, 16:240, 16:240]\n",
        "    crops.append(center)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Create horizontal flips of all 5 crops\n",
        "\n",
        "\n",
        "    #writing 5 bcoz if that is abset then at every iteration,\n",
        "    # the appended crop will also be considered --> infinite loop\n",
        "    for crop in crops[:5]:\n",
        "\n",
        "# ---------->>>>  IMPORTANT -> torch.flip() with dims=[-1] flips along the last dimension (width)\n",
        "\n",
        "\n",
        "      flipped_crop = torch.flip(crop, dims=[-1])\n",
        "      crops.append(flipped_crop)\n",
        "\n",
        "\n",
        "      #verif=ying if we have 10 crops or other count\n",
        "      assert len(crops) == 10, f\"Expected 10 crops, got {len(crops)}\"\n",
        "\n",
        "      # Verify all crops have correct shape\n",
        "      for i, crop in enumerate(crops):\n",
        "          assert crop.shape == (3, 224, 224), f\"Crop {i} has wrong shape: {crop.shape}\"\n",
        "\n",
        "    return crops  # Should return list of 10 image tensors\n"
      ],
      "metadata": {
        "id": "0dCkSlRcHMqo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def implement_pca_color_augmentation(dataset_path):\n",
        "    \"\"\"\n",
        "    Optional advanced function: Implement PCA-based color augmentation.\n",
        "    Hint: You need to collect RGB pixel values from your entire dataset,\n",
        "    compute the covariance matrix, find eigenvectors/eigenvalues,\n",
        "    then create a transform that adds random combinations of these principal components.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # This is advanced - skip if you want to focus on core architecture first\n",
        "\n",
        "    # we willlc ome back later\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "5H0kHSrXHM2u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 3: Dataset Loading\n",
        "\n",
        "**Detailed Hint:** You need to create data loaders that can efficiently feed batches of images to your network during training. Think about memory management, shuffling strategies, and how to handle different dataset formats. The original AlexNet used ImageNet, but you might start with CIFAR-10 for faster experimentation. Consider what batch size makes sense for your hardware constraints.\n"
      ],
      "metadata": {
        "id": "kwvnfoGX52fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loaders(batch_size=128):\n",
        "    \"\"\"\n",
        "    Create PyTorch DataLoaders for training and validation.\n",
        "    Hint: You need to handle the directory structure of your dataset.\n",
        "    Think about what arguments DataLoader needs for efficient training (shuffling, number of workers).\n",
        "    \"\"\"\n",
        "    train_transform, val_transform = create_basic_transforms()\n",
        "\n",
        "    # TODO: Create dataset objects using torchvision.datasets\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root = \"./data\",\n",
        "        train = True,\n",
        "        transform=train_transform,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "\n",
        "    val_dataset = datasets.CIFAR10(\n",
        "        root = \"./data\",\n",
        "        train = False,\n",
        "        transform=val_transform,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Print dataset information\n",
        "    print(f\"Training dataset size: {len(train_dataset)} images\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)} images\")\n",
        "    print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "    print(f\"Class names: {train_dataset.classes}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Create DataLoader objects\n",
        "    train_loader = DataLoader(\n",
        "        dataset = train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset = val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "    return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "aQzJA93f5ajq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def verify_data_loading(data_loader):\n",
        "    \"\"\"\n",
        "    Test function to verify your data loading works correctly.\n",
        "    Hint: Grab a batch, check the shapes, verify the data types and value ranges.\n",
        "    Print out some statistics to ensure everything looks reasonable.\n",
        "    \"\"\"\n",
        "    # TODO: Get one batch from the data loader\n",
        "\n",
        "    data_iter = iter(data_loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print shapes and data types\n",
        "    print(f\"Batch images shape: {images.shape}\")  # Should be [batch_size, 3, 224, 224]\n",
        "    print(f\"Batch labels shape: {labels.shape}\")  # Should be [batch_size]\n",
        "    print(f\"Images data type: {images.dtype}\")    # Should be torch.float32\n",
        "    print(f\"Labels data type: {labels.dtype}\")    # Should be torch.int64\n",
        "\n",
        "    # Check value ranges\n",
        "    print(f\"Image pixel value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "    print(f\"Label range: [{labels.min()}, {labels.max()}]\")\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Print shapes, min/max values, data types\n",
        "    # TODO: Maybe visualize a few images to verify augmentations work\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "p-6XwcoTWBVK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loader, val_loader = create_data_loaders()\n",
        "verify_data_loading(train_loader)\n",
        "verify_data_loading(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk65D0cxXKhb",
        "outputId": "7ddd7785-abae-4852-fda6-4e4dd91daf36"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 50000 images\n",
            "Validation dataset size: 10000 images\n",
            "Number of classes: 10\n",
            "Class names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
            "Training batches per epoch: 391\n",
            "Validation batches: 79\n",
            "Batch images shape: torch.Size([128, 3, 224, 224])\n",
            "Batch labels shape: torch.Size([128])\n",
            "Images data type: torch.float32\n",
            "Labels data type: torch.int64\n",
            "Image pixel value range: [-2.118, 2.640]\n",
            "Label range: [0, 9]\n",
            "Batch images shape: torch.Size([128, 3, 224, 224])\n",
            "Batch labels shape: torch.Size([128])\n",
            "Images data type: torch.float32\n",
            "Labels data type: torch.int64\n",
            "Image pixel value range: [-2.118, 2.640]\n",
            "Label range: [0, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def verify_data_loading(data_loader, dataset_name=\"Dataset\"):\n",
        "#     \"\"\"\n",
        "#     Test function to verify your CIFAR-10 data loading works correctly.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         data_iter = iter(data_loader)\n",
        "#         images, labels = next(data_iter)\n",
        "#         print(\"✓ Successfully loaded one batch\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"✗ Error loading batch: {e}\")\n",
        "#         return\n",
        "\n",
        "#     # Print shapes and data types\n",
        "#     print(f\"Batch images shape: {images.shape}\")  # Should be [batch_size, 3, 224, 224]\n",
        "#     print(f\"Batch labels shape: {labels.shape}\")  # Should be [batch_size]\n",
        "#     print(f\"Images data type: {images.dtype}\")    # Should be torch.float32\n",
        "#     print(f\"Labels data type: {labels.dtype}\")    # Should be torch.int64\n",
        "\n",
        "#     # Check value ranges\n",
        "#     print(f\"Image pixel value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "#     print(f\"Label range: [{labels.min()}, {labels.max()}]\")\n",
        "\n",
        "#     # Check for problematic values\n",
        "#     if torch.isnan(images).any():\n",
        "#         print(\"⚠️  WARNING: Found NaN values in images!\")\n",
        "#     if torch.isinf(images).any():\n",
        "#         print(\"⚠️  WARNING: Found infinite values in images!\")\n",
        "\n",
        "#     # Visualize sample images\n",
        "#     visualize_cifar10_samples(images, labels, data_loader.dataset, dataset_name)\n",
        "\n",
        "#     # Test multiple batch loading\n",
        "#     print(\"\\nTesting multiple batch loading...\")\n",
        "#     batch_count = 0\n",
        "#     try:\n",
        "#         for batch_idx, (batch_images, batch_labels) in enumerate(data_loader):\n",
        "#             batch_count += 1\n",
        "#             if batch_idx >= 2:  # Test first 3 batches\n",
        "#                 break\n",
        "#         print(f\"✓ Successfully loaded {batch_count} batches\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"✗ Error in batch {batch_count}: {e}\")\n",
        "\n",
        "#     print(f\"=== {dataset_name} Data Loading Verification Complete ===\\n\")"
      ],
      "metadata": {
        "id": "qUeA-tWYWbkb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 4: Local Response Normalization (LRN)\n",
        "\n",
        "**Detailed Hint:** This is AlexNet's \"secret sauce\" for competition between feature maps. You're implementing the formula from the paper where each activation gets normalized by its neighbors. Think about how to efficiently compute the sum of squares across nearby channels for each spatial location. PyTorch removed built-in LRN, so you need to implement it as a custom module. Consider the sliding window of channels and how to handle edge cases.\n"
      ],
      "metadata": {
        "id": "xU9uxsai550Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalResponseNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement Local Response Normalization as described in AlexNet paper.\n",
        "    Hint: The formula involves looking at nearby channels and computing their squared sum.\n",
        "    You need to implement the forward pass that applies the mathematical formula to each activation.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, size=5, alpha=1e-4, beta=0.75, k=2.0):\n",
        "        \"\"\"\n",
        "        Initialize LRN parameters.\n",
        "\n",
        "        Args:\n",
        "            size (int): Number of nearby channels to consider (n in paper)\n",
        "            alpha (float): Scaling parameter (α in paper)\n",
        "            beta (float): Exponent parameter (β in paper)\n",
        "            k (float): Additive constant to prevent division by zero\n",
        "        \"\"\"\n",
        "        super(LocalResponseNorm, self).__init__()\n",
        "        self.size = size\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.k = k\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply LRN to input tensor x.\n",
        "\n",
        "        Hint: x has shape [batch, channels, height, width]\n",
        "\n",
        "        For each position, you need to look at 'size' nearby channels,\n",
        "        compute the sum of their squares, then apply the normalization formula.\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, channels, height, width = x.size()\n",
        "\n",
        "        # TODO: Implement the LRN formula\n",
        "\n",
        "        # Step 1: Squaring the input\n",
        "\n",
        "        x_squared = x.pow(2)\n",
        "\n",
        "\n",
        "        #step 2:  Padding\n",
        "\n",
        "        #Step 2.1: Calculate how much padding we need:\n",
        "\n",
        "        padding = self.size // 2\n",
        "\n",
        "\n",
        "        # step 2.2: add the padding using the FANCY SYNTAX\n",
        "\n",
        "        x_squared_padded = F.pad(\n",
        "            x_squared,\n",
        "            (0, 0, 0, 0, padding, padding),\n",
        "            mode='constant',\n",
        "            value=0\n",
        "        )\n",
        "       # x_squared_padded shape: [batch, channels + 2*padding, height, width]\n",
        "\n",
        "\n",
        "        # Step 3: Sliding Window\n",
        "\n",
        "        # Step 3.1: Store the window states/conditions\n",
        "\n",
        "        windows = x_squared_padded.unfold(1, self.size, 1)\n",
        "\n",
        "\n",
        "        # Step 3.2: Calculate sum for each window condition\n",
        "\n",
        "        sum_of_squares = windows.sum(dim=1)\n",
        "\n",
        "# hence we got the \"summation\" term of equation\n",
        "\n",
        "\n",
        "        denominator = torch.pow(self.k + self.alpha * sum_of_squares, self.beta)\n",
        "        denominator = torch.clamp(denominator, min=1e-8) # --> to avoid division by zero\n",
        "\n",
        "        return x/denominator  # Return normalized tensor"
      ],
      "metadata": {
        "id": "4efyN8XT55kM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 5: AlexNet Architecture\n",
        "\n",
        "**Detailed Hint:** Now you're building the actual network described in the paper. Think about the sequence: convolutional layers extract features, pooling layers reduce spatial dimensions, fully connected layers make final classifications. Pay attention to the paper's specific numbers: kernel sizes, strides, number of filters, etc. The architecture has two main parts - feature extraction (convolutional) and classification (fully connected). Consider where dropout and LRN fit in the architecture.\n"
      ],
      "metadata": {
        "id": "lvMG4a5r6Avj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the full AlexNet architecture.\n",
        "    Hint: The paper describes 5 convolutional layers followed by 3 fully connected layers.\n",
        "    Pay attention to the specific parameters: kernel sizes, strides, padding, number of filters.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNet, self).__init__()\n",
        "\n",
        "        self.features = self._make_feature_layers()\n",
        "        self.classifier = self._make_classifier_layers(num_classes)\n",
        "\n",
        "        # TODO: Initialize weights using the strategy mentioned in the paper\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_feature_layers(self):\n",
        "        \"\"\"\n",
        "        Create the convolutional feature extraction layers.\n",
        "\n",
        "        Hint: Look at the paper's Table 1 or Figure 2 for the exact layer specifications.\n",
        "\n",
        "        Remember to include ReLU activations, LRN where specified, and MaxPooling layers.\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "\n",
        "        # TODO: Add Conv2d, ReLU, LRN, MaxPool2d in the right sequence\n",
        "\n",
        "\n",
        "  # ----------- Layer 1: Conv(11x11, 96 filters, stride 4) -> ReLU -> LRN -> MaxPool -----------\n",
        "\n",
        "\n",
        "        # Large 11x11 kernel to capture big patterns, stride=4 to reduce size quickly\n",
        "\n",
        "\n",
        "        layers.append(nn.Conv2d(\n",
        "            in_channels=3,          # RGB input\n",
        "            out_channels=96,        # 96 different pattern detectors\n",
        "            kernel_size=11,         # 11x11 sliding window\n",
        "            stride=4,               # Move 4 pixels at a time (reduces size)\n",
        "            padding=2               # Add border to maintain reasonable size\n",
        "\n",
        "        ))\n",
        "\n",
        "\n",
        "        layers.append(nn.ReLU(inplace=True))  # Activation: keep positive values only\n",
        "\n",
        "        # Add Local Response Normalization (from your Phase 4 implementation)\n",
        "        layers.append(LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2.0))\n",
        "\n",
        "        # MaxPooling: Take the maximum in each 3x3 region, stride=2\n",
        "        layers.append(nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "\n",
        "\n",
        "        # ----------- Layer 2: Conv(5x5, 256 filters, stride 1) -> ReLU -> LRN -> MaxPool  -----------\n",
        "\n",
        "\n",
        "        # Input: 96 channels -> Output: 256 feature maps\n",
        "        # Smaller 5x5 kernel for more detailed patterns\n",
        "\n",
        "\n",
        "        # We implement the FULL network on one device\n",
        "        layers.append(nn.Conv2d(96, 256, kernel_size=5, padding=2))\n",
        "        #                       ↑    ↑\n",
        "        #                       96   256\n",
        "        #                       │    └─ Total output channels\n",
        "        #                       └─ Total input channels (96, not 48) --> bcoz 48 are split across 2 GPUs according to the paper.\n",
        "\n",
        "\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        layers.append(LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2.0))\n",
        "\n",
        "        layers.append(nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "\n",
        "\n",
        "\n",
        "        # ----------- Layer 3: Conv(3x3, 384 filters, stride 1) -> ReLU -----------\n",
        "\n",
        "\n",
        "        # Input: 256 channels -> Output: 384 feature maps\n",
        "        # Even smaller 3x3 kernel for fine details\n",
        "\n",
        "\n",
        "        layers.append(nn.Conv2d(256, 384, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "\n",
        "        # ----------- Layer 4: Conv(3x3, 384 filters, stride 1) -> ReLU -----------\n",
        "\n",
        "        # Input: 384 channels -> Output: 384 feature maps\n",
        "        # Same size, just processing the features further\n",
        "\n",
        "        layers.append(nn.Conv2d(384, 384, kernel_size=3, padding=1))\n",
        "\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "       # ----------- Layer 5: Conv(3x3, 256 filters, stride 1) -> ReLU -> MaxPool -----------\n",
        "\n",
        "        # Input: 384 channels -> Output: 256 feature maps\n",
        "        # Final feature extraction layer\n",
        "        layers.append(nn.Conv2d(384, 256, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _make_classifier_layers(self, num_classes):\n",
        "        \"\"\"\n",
        "        Create the fully connected classification layers.\n",
        "        Hint: The paper mentions 3 fully connected layers with specific dimensions.\n",
        "        Don't forget dropout for regularization - where should it be applied?\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "\n",
        "        # TODO: Add Linear layers with appropriate input/output dimensions\n",
        "\n",
        "        # TODO: Add ReLU activations and Dropout where appropriate\n",
        "\n",
        "        # DROPOUT: Randomly turn off 50% of neurons during training\n",
        "        # This prevents overfitting - like studying with distractions to build robustness\n",
        "        layers.append(nn.Dropout(p=0.5))\n",
        "\n",
        "\n",
        "\n",
        "        # FULLY CONNECTED LAYER 1\n",
        "        # Input: 256 * 6 * 6 = 9216 features (flattened from conv layers)\n",
        "        # Output: 4096 neurons\n",
        "        layers.append(nn.Linear(256 * 6 * 6, 4096))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "        # More dropout\n",
        "        layers.append(nn.Dropout(p=0.5))\n",
        "\n",
        "\n",
        "        # FULLY CONNECTED LAYER 2\n",
        "        # Input: 4096 -> Output: 4096\n",
        "        layers.append(nn.Linear(4096, 4096))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "\n",
        "\n",
        "        # Final layer should output 'num_classes' values (no activation - handled by loss function)\n",
        "\n",
        "        # Input: 4096 -> Output: num_classes (10 for CIFAR-10, 1000 for ImageNet)\n",
        "        # No activation here - the loss function (CrossEntropy) handles it\n",
        "        layers.append(nn.Linear(4096, num_classes))\n",
        "\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize network weights as described in the paper.\n",
        "        Hint: The paper mentions specific initialization strategies for different layer types.\n",
        "        Conv layers and Linear layers might need different approaches.\n",
        "        \"\"\"\n",
        "\n",
        "        # we loop over every layer in the network and set weights\n",
        "        for module in self.modules():\n",
        "\n",
        "          # if layer is a conv layer\n",
        "\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                # Convolutional layers: Gaussian distribution with std=0.01\n",
        "\n",
        "                #weights\n",
        "                nn.init.normal_(module.weight, mean=0, std=0.01)\n",
        "\n",
        "                #bias\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "           # if layer is fc layer\n",
        "\n",
        "            elif isinstance(module, nn.Linear):\n",
        "                # Fully connected layers: Gaussian distribution with std=0.01\\\n",
        "\n",
        "              #weights\n",
        "                nn.init.normal_(module.weight, mean=0, std=0.01)\n",
        "\n",
        "                #bias\n",
        "                nn.init.constant_(module.bias, 1)  # Paper initializes FC biases to 1\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Define the forward pass through the network.\n",
        "        Hint: Data flows through features, then gets flattened, then through classifier.\n",
        "        Pay attention to tensor shapes - when do you need to reshape?\n",
        "        \"\"\"\n",
        "        # TODO: Pass through feature extraction layers\n",
        "\n",
        "\n",
        "        #step 1: feature extraction layers\n",
        "        #pass through all conv layers\n",
        "        # Input shape: [batch_size, 3, 224, 224]\n",
        "        # Output shape: [batch_size, 256, 6, 6]\n",
        "\n",
        "\n",
        "        x = self.features(x)\n",
        "\n",
        "\n",
        "        # TODO: Flatten the tensor for fully connected layers\n",
        "        # Step 2: flatten bcoz we need to convert from 4d tensor to 2d tensore\n",
        "\n",
        "       # Think: Convert from \"image with features\" to \"list of features\"\n",
        "\n",
        "\n",
        "        x.view(x.size(0), -1) # Keep batch size, flatten everything else\n",
        "\n",
        "        # New shape: [batch_size, 256*6*6] = [batch_size, 9216]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: Step 3: Pass through classifier layers\n",
        "\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OtsUJXu75bWM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 6: Training Infrastructure\n",
        "\n",
        "**Detailed Hint:** You need to set up the training loop components: loss function, optimizer, and learning rate scheduling. The paper mentions specific choices - cross-entropy loss, SGD with momentum, specific learning rates and weight decay values. Think about what each hyperparameter does and why the authors chose these values. Also consider how to track and display training progress.\n"
      ],
      "metadata": {
        "id": "LIb-0r_X6GYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_training_components(model, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Set up loss function, optimizer, and learning rate scheduler.\n",
        "\n",
        "    Hint: AlexNet paper specifies SGD with momentum, specific weight decay values.\n",
        "\n",
        "    What loss function makes sense for multi-class classification?\n",
        "    \"\"\"\n",
        "    # TODO: Define appropriate loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # TODO: Define optimizer with paper's hyperparameters\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),           # Which weights to update\n",
        "        lr=learning_rate,            # How big steps to take (learning rate)\n",
        "        momentum=0.9,                # How much to remember previous updates\n",
        "        weight_decay=0.0005          # Regularization to prevent overfitting\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Optional - create learning rate scheduler\n",
        "\n",
        "\n",
        "    # LEARNING RATE SCHEDULER - When to change learning speed\n",
        "    # AlexNet reduces LR by factor of 10 when validation error stops improving\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',          # Reduce LR when validation loss stops decreasing\n",
        "        factor=0.1,          # Multiply LR by 0.1 (reduce by factor of 10)\n",
        "        patience=10,         # Wait 10 epochs before reducing\n",
        "        verbose=True         # Print when LR changes\n",
        "    )\n",
        "\n",
        "    return criterion, optimizer, scheduler\n"
      ],
      "metadata": {
        "id": "8S3HgFrzyuU2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Execute one training epoch.\n",
        "    Hint: This is your main training loop - iterate through batches,\n",
        "    compute forward pass, calculate loss, backpropagate, update weights.\n",
        "    Track metrics like loss and accuracy for monitoring.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # TODO: Move data to appropriate device\n",
        "        # TODO: Zero gradients\n",
        "        # TODO: Forward pass\n",
        "        # TODO: Compute loss\n",
        "        # TODO: Backward pass\n",
        "        # TODO: Optimizer step\n",
        "        # TODO: Update running statistics\n",
        "\n",
        "        # Optional: Print progress every N batches\n",
        "        pass\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ],
      "metadata": {
        "id": "LDYzb5rIywkK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validate_model(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation set.\n",
        "    Hint: Similar to training but without gradient computation.\n",
        "    You might want to implement the 10-crop testing strategy here.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            # TODO: Move data to device\n",
        "            # TODO: Forward pass\n",
        "            # TODO: Compute loss\n",
        "            # TODO: Calculate accuracy\n",
        "            pass\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "YlYizrYU6DrG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 7: Main Training Loop\n",
        "\n",
        "**Detailed Hint:** This ties everything together - your main training script that orchestrates the entire process. Think about how many epochs to train, when to save checkpoints, how to handle early stopping, and what information to log. Consider what you want to track during training and how to save the best model.\n"
      ],
      "metadata": {
        "id": "Beb02V9u6K0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_alexnet(num_epochs=90, save_path=\"alexnet_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Main training function that coordinates everything.\n",
        "    Hint: This should set up all components, then run training/validation loops.\n",
        "    Consider saving checkpoints, tracking best performance, and logging progress.\n",
        "    \"\"\"\n",
        "    # TODO: Set up device, data loaders, model, training components\n",
        "    device = setup_environment()\n",
        "    train_loader, val_loader = create_data_loaders(\"path/to/dataset\")\n",
        "    model = AlexNet(num_classes=1000).to(device)\n",
        "    criterion, optimizer, scheduler = setup_training_components(model)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # TODO: Train for one epoch\n",
        "        # TODO: Validate the model\n",
        "        # TODO: Update learning rate if using scheduler\n",
        "        # TODO: Save checkpoint if this is the best model so far\n",
        "        # TODO: Print/log progress\n",
        "\n",
        "        pass\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_alexnet()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "WGjyc2tv6Iym",
        "outputId": "a2d6d10f-47ba-4600-d4a4-6f51369584a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'setup_environment' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1049705621.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrain_alexnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1049705621.py\u001b[0m in \u001b[0;36mtrain_alexnet\u001b[0;34m(num_epochs, save_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# TODO: Set up device, data loaders, model, training components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path/to/dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlexNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'setup_environment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Phase 8: Testing with 10-Crop Strategy\n",
        "\n",
        "**Detailed Hint:** Implement AlexNet's testing strategy where you extract 10 different crops from each test image and average their predictions. This is different from training where you use random crops. Think about how this averaging helps improve accuracy and robustness.\n"
      ],
      "metadata": {
        "id": "j29BPEEi6QVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_with_ten_crops(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate model using the 10-crop testing strategy from the paper.\n",
        "    Hint: For each test image, extract 10 crops, get predictions for each,\n",
        "    then average the softmax outputs before making final prediction.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            batch_predictions = []\n",
        "\n",
        "            for img in images:\n",
        "                # TODO: Extract 10 crops from this image\n",
        "                crops = extract_ten_test_crops(img)\n",
        "\n",
        "                # TODO: Get prediction for each crop\n",
        "                crop_predictions = []\n",
        "                for crop in crops:\n",
        "                    # TODO: Forward pass through model\n",
        "                    # TODO: Apply softmax to get probabilities\n",
        "                    pass\n",
        "\n",
        "                # TODO: Average the 10 predictions\n",
        "                avg_prediction = None\n",
        "                batch_predictions.append(avg_prediction)\n",
        "\n",
        "            # TODO: Calculate accuracy using averaged predictions\n",
        "            pass\n",
        "\n",
        "    final_accuracy = correct / total\n",
        "    print(f\"10-crop test accuracy: {final_accuracy:.4f}\")\n",
        "    return final_accuracy"
      ],
      "metadata": {
        "id": "S1jggrSJ6NjF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YwTUZSH36SyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}